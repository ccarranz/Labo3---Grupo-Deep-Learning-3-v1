{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los archivos y agrupamiento de ventas por período y producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sells = pd.read_csv(\"sell-in.txt\", sep = \"\\t\")\n",
    "filter_id = pd.read_csv(\"productos_a_predecir.txt\", sep = \"\\t\")\n",
    "sells = sells[sells.product_id.isin(filter_id.product_id)]\n",
    "productos = sells.product_id[sells.periodo == max(sells.periodo)]\n",
    "productos = np.unique(productos)\n",
    "df_sells = sells.groupby([\"product_id\", \"periodo\"])[\"tn\"].aggregate('sum').reset_index()\n",
    "df_sells.sort_values([\"product_id\", \"periodo\"], inplace = True)\n",
    "descripcion = pd.read_csv(r\"C:\\Users\\rodri\\OneDrive\\Documentos\\Maestria Ciencia de Datos\\Labo 3\\DataSets\\tb_productos.txt\", sep = \"\\t\")\n",
    "descripcion = descripcion[descripcion.product_id.isin(filter_id.product_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sells.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un dataset con todos los periodos para todos los productos y se realiza un join con el dataset leido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = pd.DataFrame(np.array(np.meshgrid(np.unique(df_sells.product_id), \n",
    "                                                np.unique(df_sells.periodo))).T.reshape(-1,2),\n",
    "                           columns = [\"product_id\", \"periodo\"])\n",
    "\n",
    "df_completo = df_completo.merge(df_sells, how=\"left\", on=[\"product_id\",\"periodo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se transforma el dataset de formato long a wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm = df_completo.pivot(columns=\"product_id\", values = \"tn\", index = \"periodo\")\n",
    "df_lstm.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza la función interpole para identificar los meses en los que no hubo ventas de productos existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolado = df_lstm.interpolate(axis=0,limit_area=\"inside\",limit_direction=\"both\")\n",
    "df_lstm.fillna(-1, inplace=True)\n",
    "df_lstm[(interpolado>0) & (df_lstm == -1)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un dataset indicando la antigüedad de cada producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_antiguedad = []\n",
    "nombres_antiguedad = []\n",
    "for k in df_lstm.columns:\n",
    "  df_antiguedad = df_lstm.loc[:,k]\n",
    "  antiguedad = []\n",
    "  nombre = \"antiguedad_\" + str(k)\n",
    "  if sum(df_antiguedad == -1) > 0:\n",
    "    antiguedad = [-1] * sum(df_antiguedad == -1)\n",
    "    antiguedad.extend(range(36 - sum(df_antiguedad == -1)))\n",
    "  if (np.mean(df_antiguedad[24:]) < 0.5 * np.mean(df_antiguedad[12:24])) & (len(antiguedad) == 0):\n",
    "    antiguedad = [-2] * 36\n",
    "  if len(antiguedad) == 0:\n",
    "    antiguedad = [-3] * 36\n",
    "  \n",
    "  lista_antiguedad.append(antiguedad)\n",
    "  nombres_antiguedad.append(nombre)\n",
    "\n",
    "diccionario = dict(zip(nombres_antiguedad, lista_antiguedad))\n",
    "\n",
    "df_con_antiguedad = pd.DataFrame(diccionario)\n",
    "df_con_antiguedad.index = lstm_delta.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vuelven a imputar los meses de los producto que no existían en NAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm[df_lstm == -1] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_lstm)\n",
    "df_lstm_scale = pd.DataFrame(scaler.transform(df_lstm), columns=df_lstm.columns, index = df_lstm.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación de NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza el promedio de ventas para cada mes para imputar los NA de productos que no existían."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.unique(descripcion[\"cat1\"]) :\n",
    "  for j in np.unique(descripcion[\"cat2\"]) :\n",
    "    ids = list(np.unique(descripcion[(descripcion[\"cat1\"] == i) & (descripcion[\"cat2\"] == j)].product_id))\n",
    "    promedios = pd.DataFrame(np.tile(df_lstm_scale[ids].mean(axis = 1), (len(ids), 1)).transpose(), columns = ids, index=df_lstm_scale.index)\n",
    "    df_lstm_scale[ids] = df_lstm_scale[ids].fillna(promedios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación de ventas en 201908\n",
    "\n",
    "Se imputan las ventas de este mes con un interpolación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_scale.iloc[df_lstm_scale.index == 201908,:] = np.NaN\n",
    "df_lstm_scale.interpolate(method='polynomial',order = 2, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación dataframe de ventas escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_2 = pd.DataFrame(scaler.inverse_transform(df_lstm_scale), columns=df_lstm.columns, index = df_lstm.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculo de pesos a incorporar al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos = list(df_lstm_2.loc[[201901,201902,201903],:].mean(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de DataFrame con variables delta lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = df_lstm.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean 12 variables para cada producto por la diferencia entre las ventas de un periodo con el del mes anterior al año anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(data, lag=12):\n",
    "    list_df = []\n",
    "    for i in range(1, lag + 1):\n",
    "        data_lag = data.diff(periods=i, axis=0)\n",
    "        data_lag.columns = f\"delta_{i}_\" + columnas.astype(str)\n",
    "        data_lag.iloc[0:i,:] = data_lag.iloc[12:i+12,:]\n",
    "        list_df.append(data_lag)\n",
    "    return pd.concat(list_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_delta = create_lagged_features(df_lstm_scale, lag=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de variable roll mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean variables de roll mean con el objetivo de identificar si se compró mucho o poco en los últimos meses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_features(data, mean):\n",
    "    list_df = []\n",
    "    for i in range(1, mean - 1):\n",
    "        data_mean = data.rolling(i+1, min_periods=0).sum()\n",
    "        data_mean.columns = f\"mean_{i}_\" + columnas.astype(str)\n",
    "        list_df.append(data_mean)\n",
    "    return pd.concat(list_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mean = create_mean_features(df_lstm_scale, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenación de los DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo = pd.concat([df_lstm_scale,lstm_delta,lstm_mean, df_con_antiguedad], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modificación del índice en formato datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deepar = df_modelo.copy()\n",
    "df_deepar.index = df_deepar.index.map(str) + \"01\"\n",
    "df_deepar.index = pd.to_datetime(df_deepar.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y predicción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el algoritmo DeepAR se envía el dataset completo directamente y se entrena el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PandasDataset(dict(df_deepar))\n",
    "\n",
    "estimator = DeepAREstimator(freq='M', prediction_length=2, \n",
    "                            num_layers=2, trainer_kwargs={'max_epochs':100}, \n",
    "                            hidden_size = 1000)\n",
    "\n",
    "predictor = estimator.train(train_ds, num_workers=8,one_dim_target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = predictor.predict(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se graban las predicciones en un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones.to_csv(\"LSTM_completo_1.csv\", index = False)\n",
    "pred_2 = list(prediccion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo genera predicciones para todas las series de tiempo incorporadas, por lo tanto se deben filtrar las series de ventas de productos.\n",
    "No se incorporaron variables exógenas al modelo porque se le debían pasar los valores de las variables exógenas en el período a predecir. Por lo tanto se incorporaron todas las variables como series diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo devuelve un sample de 100 prediciones de la serie en la ventana de tiempo dado. Para calcular la predicción se debe calcular el promedio de este sample. Esto permite conocer el intervalo de confianza de las predicciones realizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = list()\n",
    "for item in pred_2:\n",
    "    family = item.item_id\n",
    "    p = item.samples.mean(axis=0)[1]\n",
    "    all_preds += [pd.DataFrame({\"product_id\": [family], \"tn\": [p]})]\n",
    "all_preds = pd.concat(all_preds, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se filtran las predicciones de ventas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = all_preds[all_preds.product_id.isin(productos.astype(str))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ordenan las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds[\"product_id\"] = all_preds.product_id.astype(int)\n",
    "all_preds.sort_values(by = [\"product_id\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se revierte el escalado de los valores de las ventas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = scaler.inverse_transform(np.array(all_preds.tn).reshape(1,-1))\n",
    "\n",
    "df_prediccion = pd.DataFrame({\"product_id\" : all_preds.product_id,\"tn\" : y_final[0,:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se graban las predicciones de en un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediccion.to_csv(\"deepar_10.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
