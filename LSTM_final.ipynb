{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los archivos y agrupamiento de ventas por período y producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sells = pd.read_csv(\"sell-in.txt\", sep = \"\\t\")\n",
    "filter_id = pd.read_csv(\"productos_a_predecir.txt\", sep = \"\\t\")\n",
    "sells = sells[sells.product_id.isin(filter_id.product_id)]\n",
    "productos = sells.product_id[sells.periodo == max(sells.periodo)]\n",
    "productos = np.unique(productos)\n",
    "df_sells = sells.groupby([\"product_id\", \"periodo\"])[\"tn\"].aggregate('sum').reset_index()\n",
    "df_sells.sort_values([\"product_id\", \"periodo\"], inplace = True)\n",
    "descripcion = pd.read_csv(r\"C:\\Users\\rodri\\OneDrive\\Documentos\\Maestria Ciencia de Datos\\Labo 3\\DataSets\\tb_productos.txt\", sep = \"\\t\")\n",
    "descripcion = descripcion[descripcion.product_id.isin(filter_id.product_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sells.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un dataset con todos los periodos para todos los productos y se realiza un join con el dataset leido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = pd.DataFrame(np.array(np.meshgrid(np.unique(df_sells.product_id), \n",
    "                                                np.unique(df_sells.periodo))).T.reshape(-1,2),\n",
    "                           columns = [\"product_id\", \"periodo\"])\n",
    "\n",
    "df_completo = df_completo.merge(df_sells, how=\"left\", on=[\"product_id\",\"periodo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se transforma el dataset de formato long a wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm = df_completo.pivot(columns=\"product_id\", values = \"tn\", index = \"periodo\")\n",
    "df_lstm.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza la función interpole para identificar los meses en los que no hubo ventas de productos existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolado = df_lstm.interpolate(axis=0,limit_area=\"inside\",limit_direction=\"both\")\n",
    "df_lstm.fillna(-1, inplace=True)\n",
    "df_lstm[(interpolado>0) & (df_lstm == -1)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un dataset indicando la antigüedad de cada producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_antiguedad = []\n",
    "nombres_antiguedad = []\n",
    "for k in df_lstm.columns:\n",
    "  df_antiguedad = df_lstm.loc[:,k]\n",
    "  antiguedad = []\n",
    "  nombre = \"antiguedad_\" + str(k)\n",
    "  if sum(df_antiguedad == -1) > 0:\n",
    "    antiguedad = [-1] * sum(df_antiguedad == -1)\n",
    "    antiguedad.extend(range(36 - sum(df_antiguedad == -1)))\n",
    "  if (np.mean(df_antiguedad[24:]) < 0.5 * np.mean(df_antiguedad[12:24])) & (len(antiguedad) == 0):\n",
    "    antiguedad = [-2] * 36\n",
    "  if len(antiguedad) == 0:\n",
    "    antiguedad = [-3] * 36\n",
    "  \n",
    "  lista_antiguedad.append(antiguedad)\n",
    "  nombres_antiguedad.append(nombre)\n",
    "\n",
    "diccionario = dict(zip(nombres_antiguedad, lista_antiguedad))\n",
    "\n",
    "df_con_antiguedad = pd.DataFrame(diccionario)\n",
    "df_con_antiguedad.index = lstm_delta.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vuelven a imputar los meses de los producto que no existían en NAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm[df_lstm == -1] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_lstm)\n",
    "df_lstm_scale = pd.DataFrame(scaler.transform(df_lstm), columns=df_lstm.columns, index = df_lstm.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación de NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza el promedio de ventas para cada mes para imputar los NA de productos que no existían."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.unique(descripcion[\"cat1\"]) :\n",
    "  for j in np.unique(descripcion[\"cat2\"]) :\n",
    "    ids = list(np.unique(descripcion[(descripcion[\"cat1\"] == i) & (descripcion[\"cat2\"] == j)].product_id))\n",
    "    promedios = pd.DataFrame(np.tile(df_lstm_scale[ids].mean(axis = 1), (len(ids), 1)).transpose(), columns = ids, index=df_lstm_scale.index)\n",
    "    df_lstm_scale[ids] = df_lstm_scale[ids].fillna(promedios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación de ventas en 201908\n",
    "\n",
    "Se imputan las ventas de este mes con un interpolación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_scale.iloc[df_lstm_scale.index == 201908,:] = np.NaN\n",
    "df_lstm_scale.interpolate(method='polynomial',order = 2, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación dataframe de ventas escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_2 = pd.DataFrame(scaler.inverse_transform(df_lstm_scale), columns=df_lstm.columns, index = df_lstm.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculo de pesos a incorporar al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos = list(df_lstm_2.loc[[201901,201902,201903],:].mean(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de DataFrame con variables delta lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = df_lstm.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean 12 variables para cada producto por la diferencia entre las ventas de un periodo con el del mes anterior al año anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(data, lag=12):\n",
    "    list_df = []\n",
    "    for i in range(1, lag + 1):\n",
    "        data_lag = data.diff(periods=i, axis=0)\n",
    "        data_lag.columns = f\"delta_{i}_\" + columnas.astype(str)\n",
    "        data_lag.iloc[0:i,:] = data_lag.iloc[12:i+12,:]\n",
    "        list_df.append(data_lag)\n",
    "    return pd.concat(list_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_delta = create_lagged_features(df_lstm_scale, lag=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de variable roll mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean variables de roll mean con el objetivo de identificar si se compró mucho o poco en los últimos meses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_features(data, mean):\n",
    "    list_df = []\n",
    "    for i in range(1, mean - 1):\n",
    "        data_mean = data.rolling(i+1, min_periods=0).sum()\n",
    "        data_mean.columns = f\"mean_{i}_\" + columnas.astype(str)\n",
    "        list_df.append(data_mean)\n",
    "    return pd.concat(list_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mean = create_mean_features(df_lstm_scale, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenación de los DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo = pd.concat([df_lstm_scale,lstm_delta,lstm_mean, df_con_antiguedad], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y predicción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crearán ventas de 24 meses para el entrenamiento y se utilizarán 2 períodos de validación y 1 de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(df_modelo[0:32])\n",
    "X_validate = np.array(df_modelo[8:33])\n",
    "X_test = np.array(df_modelo[10:34])\n",
    "\n",
    "y_train = np.array(df_modelo[1:33])\n",
    "y_validate = np.array(df_modelo[10:35])\n",
    "y_test = np.array(df_modelo[12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se les da el formato correspondiente a los datos para que sean introducidos al LSTM. Se crea una función para los datos X y otra para los datos Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dataset_supervisado(array, input_length, output_length):\n",
    "\n",
    "    # Inicialización\n",
    "    X = [] # Listados que contendrán los datos de entrada y salida del modelo\n",
    "    shape = array.shape\n",
    "    if len(shape)==1: # Si tenemos sólo una serie (univariado)\n",
    "        fils, cols = array.shape[0], 1\n",
    "        array = array.reshape(fils,cols)\n",
    "    else: # Multivariado <-- <--- ¡esta parte de la función se ejecuta en este caso!\n",
    "        fils, cols = array.shape\n",
    "\n",
    "    # Generar los arreglos\n",
    "    for i in range(fils-input_length+1):\n",
    "        X.append(array[i:i+INPUT_LENGTH,0:cols])\n",
    "\n",
    "    \n",
    "    # Convertir listas a arreglos de NumPy\n",
    "    X = np.array(X)\n",
    "\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dataset_supervisado_2(array, input_length, output_length):\n",
    "\n",
    "    # Inicialización\n",
    "    X = [] # Listados que contendrán los datos de entrada y salida del modelo\n",
    "    shape = array.shape\n",
    "    if len(shape)==1: # Si tenemos sólo una serie (univariado)\n",
    "        fils, cols = array.shape[0], 1\n",
    "        array = array.reshape(fils,cols)\n",
    "    else: # Multivariado <-- <--- ¡esta parte de la función se ejecuta en este caso!\n",
    "        fils, cols = array.shape\n",
    "\n",
    "    # Generar los arreglos\n",
    "    for i in range(fils-input_length+1):\n",
    "        X.append(array[fils - i - output_length:fils - i,0:cols])\n",
    "\n",
    "    \n",
    "    # Convertir listas a arreglos de NumPy\n",
    "    X = np.array(X)\n",
    "\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utillizan las categorías cat1 y cat2 para agrupar los datos y pasarlos por el modelo. Por lo tanto, se debe realizar un loop y preparar los datos de entrenamiento y testeo para cada loop. Luego se concatenan las predicciones en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar parámetros para reproducibilidad del entrenamiento\n",
    "tf.random.set_seed(123)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "predicciones = pd.DataFrame(columns=[\"product_id\", \"tn\"])\n",
    "for i in np.unique(descripcion[\"cat1\"]) :\n",
    "  for j in np.unique(descripcion[\"cat2\"]) :\n",
    "    ids = list(np.unique(descripcion[(descripcion[\"cat1\"] == i) & (descripcion[\"cat2\"] == j)].product_id))\n",
    "    if len(ids) > 0 :\n",
    "        df = df_lstm_scale[ids]\n",
    "        lstm_delta = create_lagged_features(df_lstm_scale, lag=12)\n",
    "        lstm_mean = create_mean_features(df_lstm_scale, 6)\n",
    "        df_modelo = pd.concat([df,lstm_delta,lstm_mean], axis=1)\n",
    "        df_final = df_lstm_2[ids]\n",
    "\n",
    "        X_train = np.array(df_modelo[0:32])\n",
    "        X_validate = np.array(df_modelo[8:33])\n",
    "        X_test = np.array(df_modelo[10:34])\n",
    "\n",
    "        y_train = np.array(df[1:33])\n",
    "        y_validate = np.array(df[10:35])\n",
    "        y_test = np.array(df[12:])\n",
    "        # Crear los datasets de entrenamiento, prueba y validación y verificar sus tamaños\n",
    "        INPUT_LENGTH = 24    # Hiperparámetro\n",
    "        OUTPUT_LENGTH = 1    # Modelo multi-step\n",
    "\n",
    "        x_tr = crear_dataset_supervisado(X_train, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "        x_vl = crear_dataset_supervisado(X_validate, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "        x_ts = crear_dataset_supervisado(X_test, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "\n",
    "        y_tr = crear_dataset_supervisado_2(y_train, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "        y_vl = crear_dataset_supervisado_2(y_validate, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "        y_ts = crear_dataset_supervisado_2(y_test, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "        # El modelo\n",
    "        N_UNITS = 500 # Tamaño del estado oculto (h) y de la celdad de memoria (c) (128)\n",
    "        INPUT_SHAPE = (x_tr.shape[1], x_tr.shape[2]) # 24 (horas) x 13 (features)\n",
    "\n",
    "        modelo = Sequential()\n",
    "        modelo.add(LSTM(N_UNITS, input_shape=INPUT_SHAPE, return_sequences=True,dropout=0.2))\n",
    "        #modelo.add(LSTM(N_UNITS, return_sequences=True))\n",
    "        modelo.add(LSTM(N_UNITS, return_sequences=True,dropout=0.2))\n",
    "        modelo.add(LSTM(N_UNITS, return_sequences=True,dropout=0.2))\n",
    "        modelo.add(LSTM(N_UNITS))\n",
    "        # Y lo único que cambia con respecto al modelo multivariado + multi-step es\n",
    "        # el tamaño deldato de salida (4 horas)\n",
    "        modelo.add(Dense(df.shape[1], activation='linear')) # activation = 'linear' pues queremos pronosticar (regresión)\n",
    "\n",
    "        # Pérdida: se usará el RMSE (root mean squared error) para el entrenamiento\n",
    "        # pues permite tener errores en las mismas unidades de la temperatura\n",
    "        def root_mean_squared_error(y_true, y_pred):\n",
    "            rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
    "            return rmse\n",
    "\n",
    "        pesos_filtro = df_lstm_2[ids]\n",
    "        pesos = list(pesos_filtro.loc[[201901,201902,201903],:].mean(axis = 0))\n",
    "        # Compilación\n",
    "        optimizador = RMSprop(learning_rate=5e-4) # 5e-5\n",
    "        modelo.compile(\n",
    "            optimizer = optimizador,\n",
    "            loss = \"MeanSquaredError\",\n",
    "            loss_weights = pesos\n",
    "        )   \n",
    "\n",
    "        EPOCHS = 100 # Hiperparámetro\n",
    "        BATCH_SIZE = 5 # Hiperparámetro\n",
    "        historia = modelo.fit(\n",
    "            x = x_tr,\n",
    "            y = y_tr,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            epochs = EPOCHS,\n",
    "            validation_data = (x_vl, y_vl),\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        scaler_final = StandardScaler()\n",
    "\n",
    "        scaler_final = scaler_final.fit(df_modelo)\n",
    "        df_escalas = pd.DataFrame({\"product_id\" : df_modelo.columns,\"Mean\" : scaler_final.mean_,\"STD\" : scaler_final.scale_})\n",
    "\n",
    "        #X_final = scaler_final.transform(df_lstm[-12:])\n",
    "        X_final = scaler_final.transform(df_modelo[-24:])\n",
    "\n",
    "        X_final = crear_dataset_supervisado(X_final, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "        # 1. Generar las predicciones sobre el set de prueba\n",
    "        y_final = modelo.predict(X_final, verbose=0)\n",
    "        \n",
    "        scaler_2 = StandardScaler()\n",
    "\n",
    "        scaler_2 = scaler_2.fit(df_lstm[ids])\n",
    "        # 2. Realizar la transformación inversa de las predicciones para llevar sus\n",
    "        # valores a la escala original\n",
    "        y_final = scaler_2.inverse_transform(y_final)\n",
    "\n",
    "        df_prediccion = pd.DataFrame({\"product_id\" : df_final.columns,\"tn\" : y_final[0,:]})\n",
    "\n",
    "        predicciones = pd.concat([predicciones, df_prediccion], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se graban las predicciones en un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones.to_csv(\"LSTM_completo_1.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
